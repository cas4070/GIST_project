{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import random\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score # AUC score\n",
    "from sklearn.metrics import average_precision_score # AUPR score\n",
    "from sklearn.metrics import precision_recall_fscore_support # precision, recall\n",
    "from imblearn.metrics import sensitivity_specificity_support # sensitivity, specificity\n",
    "from sklearn.metrics import roc_curve # to draw auc curve\n",
    "from sklearn.metrics import precision_recall_curve # to draw aupr curve\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#2017.5.2.\n",
    "path = \"/DAS_Storage1/aschoi/data/Drug_Repositioning/8_new_training/2_similarity_based-PREDICT/\"\n",
    "tn = pd.read_table(path + \"4_5_tn_local_norm_1_1_descriptor.tsv\")\n",
    "no_tn = pd.read_table(path + \"4_5_2_noTn_only_local_norm_1_1_descriptor.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#2017.5.2. independent dataset\n",
    "path = \"/DAS_Storage1/aschoi/data/Drug_Repositioning/8_new_training/2_similarity_based-PREDICT/\"\n",
    "indep = pd.read_table(path+\"3_2_indep_descriptor.tsv\")\n",
    "indep_x = indep.values[:, 3:].astype(float)\n",
    "indep_y = indep.values[:,2].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open(\"/home/share/aschoi/nas/users/asolchoi/data/Drug_Repositioning/8_new_training/5_final/tn+noise_1by1_idex.txt\", \"r\")\n",
    "indices = list()\n",
    "while True:\n",
    "    line=f.readline()\n",
    "    if not line : break\n",
    "    indices.append(line.lstrip('[').rstrip(']\\n').split(', '))\n",
    "f.close()\n",
    "\n",
    "lindices = list()\n",
    "for i in indices:\n",
    "    tmp = list()\n",
    "    for j in i:\n",
    "        tmp.append(int(j))\n",
    "    lindices.append(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "idx = lindices[1]\n",
    "filtered = no_tn.filter(items = idx, axis=0) # 뽑은 index만큼 filter\n",
    "noise_tn = pd.concat([tn,filtered])\n",
    "\n",
    "x_whole_data = noise_tn[noise_tn.columns.values[3:].tolist()].values\n",
    "y_whole_data = noise_tn[\"association\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pick random noise by 1:1 herbal compounds (P:TN:Noise = 1:1:1 --> P:N = 1:2)\n",
    "# SVM with RBF Kernel\n",
    "print datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "path = '/home/share/aschoi/nas/users/asolchoi/data/Drug_Repositioning/8_new_training/5_final/'\n",
    "with open(path + \"6_herbal_tn+noise_1by1.txt\", 'w') as fd :\n",
    "    user_estimator = 300\n",
    "    i=1\n",
    "    \n",
    "    fd.write(\"<{} : independent>\\r\\n\".format(i))\n",
    "    \n",
    "    fp_results = dict()\n",
    "    \n",
    "    classifier = RandomForestClassifier(n_estimators=user_estimator, n_jobs=-1, class_weight='balanced')\n",
    "    classifier.fit(x_whole_data, y_whole_data)\n",
    "    indep_y_predicted_proba = classifier.predict_proba(herbal_x)\n",
    "    indep_y_predicted_label = classifier.predict(herbal_x)\n",
    "    fp_results['predicted_proba'] = indep_y_predicted_proba\n",
    "    fp_results['Predicted_label'] = indep_y_predicted_label\n",
    "    fp_results['y_true'] = herbal_y\n",
    "    \n",
    "    #write_output(fd, folds_results, i, user_estimator)\n",
    "#li_noise1.append(predicted_results(fp_results))\n",
    "print datetime.now().strftime('%Y-%m-%d %H:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params = {'k': k, 'kernel': 'rbf', 'user_c': i, 'degree':3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-8-eef8ed9b3241>, line 12)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-8-eef8ed9b3241>\"\u001b[1;36m, line \u001b[1;32m12\u001b[0m\n\u001b[1;33m    classifier = SVC(C=params['user_c'], n_jobs=-1, kernel = params['kernel', degree = params['degree']]) #n_jobs=-1 이면 모든 node 쓰는것, 신경쓰이면 30개 정도.\u001b[0m\n\u001b[1;37m                                                                                     ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def SVM_K_fold_graph(X, y, params):\n",
    "       \n",
    "    skf = StratifiedKFold(n_splits=params['k'], shuffle=True) # n_splits = k (k fold라서.), pos:neg의 비율을 고려해서 k 개의 subgroup으로 나누어줌.\n",
    "    folds_results = {'acc':[], 'auc':[], 'aupr':[], 'confusion_matrix':[], 'sn':[], 'sp':[], 'precision':[], 'recall':[]}\n",
    "    draw_results = {'fpr':[], 'tpr':[], 'precision_vec':[], 'recall_vec':[]}\n",
    "    for training_index, validation_index in skf.split(X, y):\n",
    "        x_training_set = X[training_index]\n",
    "        y_training_set = y[training_index]\n",
    "        x_validation_set = X[validation_index]\n",
    "        y_validation_set = y[validation_index]\n",
    "        \n",
    "        classifier = SVC(C=params['user_c'], n_jobs=-1, kernel = params['kernel'], degree = params['degree']) #n_jobs=-1 이면 모든 node 쓰는것, 신경쓰이면 30개 정도.\n",
    "        classifier.fit(x_training_set, y_training_set)\n",
    "        \n",
    "        y_predicted_proba = classifier.predict_proba(x_validation_set) # [0에 대한 확률, 1에 대한 확률], shpae = [n_samples, n_class]\n",
    "        y_predicted_label = classifier.predict(x_validation_set) # 예측된 label을 보여줌, shpae = [n_samples]\n",
    "        \n",
    "        # Accuracy \n",
    "        current_acc = classifier.score(x_validation_set, y_validation_set)\n",
    "        folds_results['acc'].append(current_acc)\n",
    "\n",
    "        # AUC\n",
    "        current_auc = roc_auc_score(y_validation_set, y_predicted_proba[:,1])\n",
    "        folds_results['auc'].append(current_auc)\n",
    "        \n",
    "        # Sensitivity, Specificity\n",
    "        sn, sp, support = sensitivity_specificity_support(y_validation_set, y_predicted_label)\n",
    "        folds_results['sn'].append(sn)\n",
    "        folds_results['sp'].append(sp)\n",
    "        \n",
    "        # AUPR\n",
    "        current_aupr = average_precision_score(y_validation_set, y_predicted_proba[:,1])\n",
    "        folds_results['aupr'].append(current_aupr)\n",
    "        \n",
    "        # Precision, Recall\n",
    "        precision, recall, _, _ =  precision_recall_fscore_support(y_validation_set, y_predicted_label, average = 'binary')\n",
    "        folds_results['precision'].append(precision)\n",
    "        folds_results['recall'].append(recall)\n",
    "        \n",
    "        # Confusion Matrix\n",
    "        current_confusion_matrix = confusion_matrix(y_validation_set,y_predicted_label)\n",
    "        folds_results['confusion_matrix'].append(current_confusion_matrix)\n",
    "        \n",
    "        #draw graph\n",
    "        fpr, tpr, thresholds = roc_curve(y_validation_set, y_predicted_proba[:, 1], pos_label=1)\n",
    "        draw_results['fpr'].append(fpr)\n",
    "        draw_results['tpr'].append(tpr)\n",
    "        \n",
    "        precision_vec, recall_vec, _ = precision_recall_curve(y_validation_set, y_predicted_proba[:, 1])\n",
    "        draw_results['precision_vec'].append(precision_vec)\n",
    "        draw_results['recall_vec'].append(recall_vec)\n",
    "        \n",
    "    return folds_results, draw_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
